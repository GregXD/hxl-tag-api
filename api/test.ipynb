{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hdx.utilities.easy_logging import setup_logging\n",
    "from hdx.hdx_configuration import Configuration\n",
    "from hdx.data.dataset import Dataset\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import itertools \n",
    "import pickle\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from fastText import load_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk import ngrams\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"try.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stopWords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_model = 'wiki.en.bin'\n",
    "fmodel = load_model(fasttext_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_cols(lst):\n",
    "    return [word.lower() for word in lst]\n",
    "\n",
    "\n",
    "def remove_chars(lst):\n",
    "\n",
    "    cleaned = [re.sub('\\s+', ' ', mystring).strip() for mystring in lst]\n",
    "    cleaned = [re.sub(r'[[^A-Za-z0-9\\s]+]', ' ', mystr) for mystr in cleaned]\n",
    "    cleaned = [mystr.replace('_', ' ') for mystr in cleaned]\n",
    "    return cleaned\n",
    "\n",
    "def clean_cols(data):\n",
    "    data = lower_cols(data)\n",
    "    data = remove_chars(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(pandas_dataset, df_target):\n",
    "    if (not pandas_dataset.empty):\n",
    "    \torganization = 'HDX'   #Replace if datasets contains organization\n",
    "    \theaders = list(pandas_dataset.columns.values)\n",
    "    \theaders = clean_cols(headers)\n",
    "    for i in range(len(headers)):\n",
    "        try:\n",
    "            dic = {'Header': headers[i], \n",
    "                   'Data': list(pandas_dataset.iloc[1:, i]), \n",
    "                   'Relative Column Position': (i+1) / len(pandas_dataset.columns), \n",
    "                   'Organization': organization,\n",
    "                   'Index': i}\n",
    "            df_target.loc[len(df_target)] = dic\n",
    "        except:\n",
    "            raise Exception(\"Error: arguments not matched\")\n",
    "\n",
    "    df_result = transform_vectorizers(df_target)\n",
    "    return df_result\n",
    "\n",
    "def transform_vectorizers(df_target):\n",
    "    cols = ['Header_embedding', 'Organization_embedded', 'BOW_counts', 'ngrams_counts']\n",
    "    df = pd.DataFrame(columns = cols)\n",
    "    long_string = []\n",
    "    for i in df_target['Data']:\n",
    "        result_by_tag = word_extract(i)\n",
    "        holder_list = ''.join(result_by_tag)\n",
    "        long_string.append(holder_list)\n",
    "    bag_vectorizer = CountVectorizer()\n",
    "    corpus = long_string\n",
    "    X_vecs_bag = bag_vectorizer.fit_transform(corpus)\n",
    "    df['BOW_counts'] = [item for item in X_vecs_bag.toarray()]\n",
    "    ngrams = generate_n_grams(df_target['Header'], 3)\n",
    "    ngrams_vectorizer = CountVectorizer(tokenizer=lambda doc: doc, lowercase=False)\n",
    "    X_vec_grams = ngrams_vectorizer.fit_transform(ngrams)\n",
    "    df = df.iloc[:-2,:]\n",
    "    df['ngrams_counts'] = [item for item in X_vec_grams.toarray()]\n",
    "    df['Header_embedding'] = df_target['Header'].astype(str).apply(fmodel.get_sentence_vector)\n",
    "    df['Organization_embedded'] = df_target['Organization'].astype(str).apply(fmodel.get_sentence_vector)\n",
    "    cols = ['Header_embedding', 'Organization_embedded', 'BOW_counts', 'ngrams_counts']\n",
    "    df['features_combined'] = df[cols].values.tolist()\n",
    "#     print(df['features_combined'][4])\n",
    "#     df['features_combined'] = df['features_combined'].apply(lambda x: np.concatenate(x, axis=None))\n",
    "    df['features_combined'] = df['features_combined'].apply(lambda x: [val for item in x for val in item])\n",
    "    df.dropna()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(data_lst):\n",
    "    #remove stopwords from the data including 'the', 'and' etc.\n",
    "    wordsFiltered = []\n",
    "    for w in data_lst:\n",
    "        if w not in stopWords:\n",
    "            wordsFiltered.append(w)\n",
    "    return wordsFiltered\n",
    "\n",
    "def word_extract(row):\n",
    "    ignore = ['nan']\n",
    "    no_white = [i.lstrip() for i in row if i not in ignore and not isinstance(i, float)]\n",
    "    cleaned_text = [w.lower() for w in no_white if w not in ignore]\n",
    "    return cleaned_text\n",
    "\n",
    "def allowed_file(filename):\n",
    "    return '.' in filename and \\\n",
    "           filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS \n",
    "\n",
    "from nltk import ngrams\n",
    "def generate_n_grams(data_lst, n):\n",
    "    # cleaned = remove_chars(list(data_lst))\n",
    "    # cleaned = clean_cols(cleaned)\n",
    "    cleaned = remove_stop_words(data_lst)\n",
    "    #make sure that n_grams 'refresh' when a new dataset is encountered!!!!   \n",
    "    return list(ngrams(cleaned, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dataset = preprocess(df, \n",
    "                pd.DataFrame(columns=['Header','Data','Relative Column Position','Organization','Index']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pickle.load(open(\"model.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dataset['features_combined'] = processed_dataset['features_combined'].apply(lambda x: x[0:300])\n",
    "processed_dataset['features_combined'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(list(processed_dataset['features_combined']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
