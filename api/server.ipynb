{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from flask import Flask, redirect, request, render_template, url_for, send_from_directory, make_response\n",
    "from werkzeug.wrappers import Request, Response\n",
    "from werkzeug.serving import run_simple\n",
    "from jsonrpc import JSONRPCResponseManager, dispatcher\n",
    "import os\n",
    "import hashlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from werkzeug.utils import secure_filename\n",
    "import pickle\n",
    "import re\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from fastText import load_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "from flask_cors import CORS\n",
    "from nltk import ngrams\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.24.2'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\vanis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stopWords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Install fastText by doing:<br>\n",
    "#git clone https://github.com/facebookresearch/fastText.git<br>\n",
    "#cd fastText<br>\n",
    "#pip install .<br>\n",
    "#'wiki.en.bin' needs to be in the same directory as server.py (can be downloaded from <br>\n",
    "#https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.en.zip)<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fasttext_model = 'wiki.en.bin'\n",
    "fmodel = load_model(fasttext_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "UPLOAD_FOLDER = '\\datasets'\n",
    "ALLOWED_EXTENSIONS_CSV = set(['csv'])\n",
    "ALLOWED_EXTENSIONS_JSON = set(['json'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# app = Flask(__name__)\n",
    "# # CORS(app)\n",
    "# app.config['UPLOAD_FOLDER'] = os.path.join(app.instance_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lower_cols(lst):\n",
    "    #convert data to lowercases\n",
    "    #QUESTION: will I miss any important information? \n",
    "    return [word.lower() for word in lst if isinstance(word,str)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_chars(lst):\n",
    "    #remove punctuation characters such as \",\", \"(\", \")\", \"\"\", \":\", \"/\", and \".\"\n",
    "    #NOTE: PRESERVES WHITE SPACE.\n",
    "    #QUESTION: any other characters we should be aware of? Is this a good idea? I'm inspecting each word individually.\n",
    "    #Any potential pitfalls? \n",
    "    cleaned = [re.sub('\\s+', ' ', mystring).strip() for mystring in lst]\n",
    "    cleaned = [re.sub(r'[[^A-Za-z0-9\\s]+]', ' ', mystr) for mystr in cleaned]\n",
    "    cleaned = [mystr.replace('_', ' ') for mystr in cleaned]\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_cols(data):\n",
    "    data = lower_cols(data)\n",
    "    data = remove_chars(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fill_empty_cols(df):\n",
    "    empty_cols = []\n",
    "    for i in df.columns.values:\n",
    "        if (len(df[i].dropna()) == 0):\n",
    "            df.at[2,i] = 1\n",
    "            empty_cols.append(df.columns.get_loc(i))\n",
    "    return df, empty_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(pandas_dataset, df_target):\n",
    "    if (not pandas_dataset.empty):\n",
    "        organization = 'HDX'   #Replace if datasets contains organization\n",
    "        pandas_dataset.dropna(how = 'all', inplace = True)\n",
    "        pandas_dataset, empty_cols = fill_empty_cols(pandas_dataset.iloc[range(1,len(pandas_dataset))])\n",
    "        print(empty_cols)\n",
    "#         pandas_dataset.dropna(axis=1, how = 'all', subset=range(1,len(pandas_dataset)), inplace = True)\n",
    "        headers = list(pandas_dataset.columns.values)        \n",
    "        headers = clean_cols(headers)\n",
    "    for i in range(len(headers)):\n",
    "        try:\n",
    "            dic = {'Header': headers[i], \n",
    "                   'Data': list(pandas_dataset.iloc[1:, i]), \n",
    "                   'Relative Column Position': (i+1) / len(pandas_dataset.columns), \n",
    "                   'Organization': organization,\n",
    "                   'Index': i}\n",
    "            df_target.loc[len(df_target)] = dic\n",
    "        except:\n",
    "            raise Exception(\"Error: arguments not matched\")\n",
    "    df_result = transform_vectorizers(df_target)\n",
    "    return df_result, empty_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_vectorizers(df_target):\n",
    "    number_of_data_point_to_vectorize = 7\n",
    "    cols = ['Header_embedding', 'Organization_embedded', 'features_combined']\n",
    "    df = pd.DataFrame(columns = cols)\n",
    "    df_target, number_of_data_point_to_vectorize = embedded_datapoints(df_target, 7)\n",
    "    df['data_combined'] = df_target.loc[:, 'embedded_datapoint0': 'embedded_datapoint' \n",
    "                                                           + str(number_of_data_point_to_vectorize-1)].values.tolist()\n",
    "    df['data_combined'] = df['data_combined'].apply(lambda x: [val for item in x for val in item])\n",
    "    df['Header_embedding'] = df_target['Header'].astype(str).apply(fmodel.get_sentence_vector)\n",
    "    df['Organization_embedded'] = df_target['Organization'].astype(str).apply(fmodel.get_sentence_vector)\n",
    "    cols = ['Header_embedding', 'Organization_embedded', 'data_combined']\n",
    "    df['features_combined'] = df[cols].values.tolist()\n",
    "    df['features_combined'] = df['features_combined'].apply(lambda x: [val for item in x for val in item])\n",
    "    diff = 2934 - len(df['features_combined'][0])\n",
    "    for i in range(len(df)):\n",
    "        for j in range(diff):\n",
    "            df['features_combined'][i].append(0)\n",
    "    df = df.dropna()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def separate_words(series): \n",
    "    #each series is a long string that contains all the data\n",
    "    lst = []\n",
    "    cleanlist = [str(x) for x in series if str(x) != 'nan']\n",
    "    for i in cleanlist:\n",
    "        lst = re.split(r\"\\W+\", i)\n",
    "        lst.extend(list(filter(None, lst)))\n",
    "    return lst\n",
    "    \n",
    "def vectorize_n_datapoints(df, number_of_datapoints_to_vectorize = 7):\n",
    "#     print(df['Data'].head())\n",
    "#     print(df['Data'].iloc[0])\n",
    "#     for i in range(len(df['Data'])):\n",
    "#         df['Data_separated'].iloc[0] = separate_words(df['Data'].iloc[0])\n",
    "    df['Data_separated'] = df['Data'].apply(separate_words)\n",
    "    if (number_of_datapoints_to_vectorize > len(df['Data_separated'][0])):\n",
    "        number_of_datapoints_to_vectorize = len(df['Data_separated'][0])\n",
    "    for i in range(number_of_datapoints_to_vectorize):\n",
    "        df['datapoint' + str(i)] = df['Data_separated'].str[i]\n",
    "    return df, number_of_datapoints_to_vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def embedded_datapoints(df, number_of_data_point_to_vectorize=7):\n",
    "    df, number_of_data_point_to_vectorize = vectorize_n_datapoints(df)\n",
    "    for i in range(number_of_data_point_to_vectorize):\n",
    "        \n",
    "        df['embedded_datapoint' + str(i)] = df['datapoint' + str(i)].map(lambda x: fmodel.get_sentence_vector(str(x)))\n",
    "    return df, number_of_data_point_to_vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_stop_words(data_lst):\n",
    "    #remove stopwords from the data including 'the', 'and' etc.\n",
    "    wordsFiltered = []\n",
    "    for w in data_lst:\n",
    "        if w not in stopWords:\n",
    "            wordsFiltered.append(w)\n",
    "    return wordsFiltered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_extract(row):\n",
    "    ignore = ['nan']\n",
    "    no_white = [i.lstrip() for i in row if i not in ignore and not (isinstance(i, float) or isinstance(i,int))]\n",
    "    cleaned_text = [w.lower() for w in no_white if w not in ignore]\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def allowed_file_csv(filename):\n",
    "    return '.' in filename and \\\n",
    "           filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS_CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def allowed_file_json(filename):\n",
    "    return '.' in filename and \\\n",
    "            filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS_JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_n_grams(data_lst, n):\n",
    "    # cleaned = remove_chars(list(data_lst))\n",
    "    # cleaned = clean_cols(cleaned)\n",
    "    cleaned = remove_stop_words(data_lst)\n",
    "    #make sure that n_grams 'refresh' when a new dataset is encountered!!!!   \n",
    "    return list(ngrams(cleaned, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_dataset = pd.read_csv('test_files\\\\assessment_registry_mozambique_09042019_v3.csv', na_values=['nan',' nan'])\n",
    "input_headers = input_dataset.columns.values\n",
    "# input_dataset = input_dataset.rename(columns=input_dataset.iloc[0]).drop(input_dataset.index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 14, 21, 23, 24, 25, 28, 29]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vanis\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:251: UserWarning: Trying to unpickle estimator LabelBinarizer from version 0.20.2 when using version 0.20.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "C:\\Users\\vanis\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:251: UserWarning: Trying to unpickle estimator MLPClassifier from version 0.20.2 when using version 0.20.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Header</th>\n",
       "      <th>Predicted tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Title</td>\n",
       "      <td>country</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Location</td>\n",
       "      <td>country</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Other location</td>\n",
       "      <td>country</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Leading Organizations</td>\n",
       "      <td>x_applicants</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Participating Organization(s)</td>\n",
       "      <td>country</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Cluster(s)/Sector(s)</td>\n",
       "      <td>sector</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Start date</td>\n",
       "      <td>date</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>End date</td>\n",
       "      <td>date</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Status</td>\n",
       "      <td>status</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Collection Method(s)</td>\n",
       "      <td>x_applicants</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Frequency</td>\n",
       "      <td>No Prediction. Column only had missing values</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Key findings</td>\n",
       "      <td>x_decisions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Methodology</td>\n",
       "      <td>x_decisions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Sample size</td>\n",
       "      <td>affected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Broken handler search_api_index_default_node_i...</td>\n",
       "      <td>No Prediction. Column only had missing values</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Subject/Objective</td>\n",
       "      <td>country</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Population Type(s)</td>\n",
       "      <td>affected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Unit(s) of Measurement</td>\n",
       "      <td>country</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Report Accessibility</td>\n",
       "      <td>x_applicants</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Report File</td>\n",
       "      <td>country</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Report URL</td>\n",
       "      <td>x_decisions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Report Instructions</td>\n",
       "      <td>No Prediction. Column only had missing values</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Questionnaire Accessibility</td>\n",
       "      <td>affected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Questionnaire File</td>\n",
       "      <td>No Prediction. Column only had missing values</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Questionnaire URL</td>\n",
       "      <td>No Prediction. Column only had missing values</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Questionnaire Instructions</td>\n",
       "      <td>No Prediction. Column only had missing values</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Accessibility</td>\n",
       "      <td>geo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Data File</td>\n",
       "      <td>country</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Data URL</td>\n",
       "      <td>No Prediction. Column only had missing values</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Data Instructions</td>\n",
       "      <td>No Prediction. Column only had missing values</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Header  \\\n",
       "0                                               Title   \n",
       "1                                            Location   \n",
       "2                                      Other location   \n",
       "3                               Leading Organizations   \n",
       "4                       Participating Organization(s)   \n",
       "5                                Cluster(s)/Sector(s)   \n",
       "6                                          Start date   \n",
       "7                                            End date   \n",
       "8                                              Status   \n",
       "9                                Collection Method(s)   \n",
       "10                                          Frequency   \n",
       "11                                       Key findings   \n",
       "12                                        Methodology   \n",
       "13                                        Sample size   \n",
       "14  Broken handler search_api_index_default_node_i...   \n",
       "15                                  Subject/Objective   \n",
       "16                                 Population Type(s)   \n",
       "17                             Unit(s) of Measurement   \n",
       "18                               Report Accessibility   \n",
       "19                                        Report File   \n",
       "20                                         Report URL   \n",
       "21                                Report Instructions   \n",
       "22                        Questionnaire Accessibility   \n",
       "23                                 Questionnaire File   \n",
       "24                                  Questionnaire URL   \n",
       "25                         Questionnaire Instructions   \n",
       "26                                      Accessibility   \n",
       "27                                          Data File   \n",
       "28                                           Data URL   \n",
       "29                                  Data Instructions   \n",
       "\n",
       "                                    Predicted tag  \n",
       "0                                         country  \n",
       "1                                         country  \n",
       "2                                         country  \n",
       "3                                    x_applicants  \n",
       "4                                         country  \n",
       "5                                          sector  \n",
       "6                                            date  \n",
       "7                                            date  \n",
       "8                                          status  \n",
       "9                                    x_applicants  \n",
       "10  No Prediction. Column only had missing values  \n",
       "11                                    x_decisions  \n",
       "12                                    x_decisions  \n",
       "13                                       affected  \n",
       "14  No Prediction. Column only had missing values  \n",
       "15                                        country  \n",
       "16                                       affected  \n",
       "17                                        country  \n",
       "18                                   x_applicants  \n",
       "19                                        country  \n",
       "20                                    x_decisions  \n",
       "21  No Prediction. Column only had missing values  \n",
       "22                                       affected  \n",
       "23  No Prediction. Column only had missing values  \n",
       "24  No Prediction. Column only had missing values  \n",
       "25  No Prediction. Column only had missing values  \n",
       "26                                            geo  \n",
       "27                                        country  \n",
       "28  No Prediction. Column only had missing values  \n",
       "29  No Prediction. Column only had missing values  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#                 # process the untagged dataset\n",
    "processed_dataset, empty_cols = preprocess(input_dataset, \n",
    "                               pd.DataFrame(columns=['Header','Data','Relative Column Position','Organization','Index']))\n",
    "model = pickle.load(open(\"model.pkl\", \"rb\")) #Model needs be named model.pkl\n",
    "output_dataset = pd.DataFrame(data = model.predict(list(processed_dataset['features_combined'])))\n",
    "output_dataset.loc[empty_cols,0] = 'No Prediction. Column only had missing values'\n",
    "output_dataset.insert(loc=0, column='Header', value=input_headers)\n",
    "output_dataset.rename(index=str, columns={0: \"Predicted tag\"}, inplace=True)\n",
    "output_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# @app.route('/', methods=['GET','POST'])\n",
    "# def upload_file():\n",
    "#     if request.method == 'POST':\n",
    "#         # check if the post request has the file part\n",
    "#         if 'file' not in request.files:\n",
    "#             flash('No file part')\n",
    "#             return redirect(request.url)\n",
    "#         file = request.files['file']\n",
    "        \n",
    "#         if file.filename == '':\n",
    "#             # flash('No selected file')\n",
    "#             return redirect(request.url)\n",
    "#         # file.save(os.getcwd())\n",
    "#         if file and allowed_file_csv(file.filename):\n",
    "#             filename = secure_filename(file.filename)\n",
    "#             input_dataset = pd.read_csv(file)\n",
    "                \n",
    "#         if file and allowed_file_json(file.filename):\n",
    "#             # filename = secure_filename(file.filename)\n",
    "#             input_dataset = pd.read_json(file)\n",
    "#             input_dataset = input_dataset.rename(columns=input_dataset.iloc[0]).drop(input_dataset.index[0])\n",
    "#                 # process the untagged dataset\n",
    "#         processed_dataset = preprocess(input_dataset, \n",
    "#             pd.DataFrame(columns=['Header','Data','Relative Column Position','Organization','Index']))\n",
    "#         model = pickle.load(open(\"model.pkl\", \"rb\")) #Model needs be named model.pkl, preferably using version 0.20.3\n",
    "#         output_dataset = pd.DataFrame(data = model.predict(list(processed_dataset['features_combined'])))\n",
    "#         resp = make_response(output_dataset.to_csv())\n",
    "#         resp.headers[\"Content-Disposition\"] = \"attachment; filename=export.csv\"\n",
    "#         resp.headers[\"Content-Type\"] = \"text/csv\"\n",
    "#         return resp\n",
    "        \n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    return \n",
    "<br>\n",
    "    <!doctype html><br>\n",
    "    <title>Upload new File</title><br>\n",
    "    <h1>Upload new File (only CSV and JSON files accepted)</h1><br>\n",
    "    <form method=post enctype=multipart/form-data><br>\n",
    "      <input type=file name=file><br>\n",
    "      <input type=submit value=Upload><br>\n",
    "    # <form method=post><br>\n",
    "    #   <input name=text><br>\n",
    "    #   <input type=submit><br>\n",
    "    </form><br>\n",
    "  \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# if __name__ == '__main__':\n",
    "#      app.run(debug=True)\n",
    "     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
