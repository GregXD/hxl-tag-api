{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, redirect, request, render_template, url_for, send_from_directory, make_response\n",
    "from werkzeug.wrappers import Request, Response\n",
    "from werkzeug.serving import run_simple\n",
    "from jsonrpc import JSONRPCResponseManager, dispatcher\n",
    "import os\n",
    "import hashlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from werkzeug.utils import secure_filename\n",
    "import pickle\n",
    "import re\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from fastText import load_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "from flask_cors import CORS\n",
    "from nltk import ngrams\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\vanis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stopWords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Install fastText by doing:<br>\n",
    "#git clone https://github.com/facebookresearch/fastText.git<br>\n",
    "#cd fastText<br>\n",
    "#pip install .<br>\n",
    "#'wiki.en.bin' needs to be in the same directory as server.py (can be downloaded from <br>\n",
    "#https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.en.zip)<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_model = 'wiki.en.bin'\n",
    "fmodel = load_model(fasttext_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "UPLOAD_FOLDER = '\\datasets'\n",
    "ALLOWED_EXTENSIONS_CSV = set(['csv'])\n",
    "ALLOWED_EXTENSIONS_JSON = set(['json'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# app = Flask(__name__)\n",
    "# # CORS(app)\n",
    "# app.config['UPLOAD_FOLDER'] = os.path.join(app.instance_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_cols(lst):\n",
    "    #convert data to lowercases\n",
    "    #QUESTION: will I miss any important information? \n",
    "    return [word.lower() for word in lst if isinstance(word,str)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_chars(lst):\n",
    "    #remove punctuation characters such as \",\", \"(\", \")\", \"\"\", \":\", \"/\", and \".\"\n",
    "    #NOTE: PRESERVES WHITE SPACE.\n",
    "    #QUESTION: any other characters we should be aware of? Is this a good idea? I'm inspecting each word individually.\n",
    "    #Any potential pitfalls? \n",
    "    cleaned = [re.sub('\\s+', ' ', mystring).strip() for mystring in lst]\n",
    "    cleaned = [re.sub(r'[[^A-Za-z0-9\\s]+]', ' ', mystr) for mystr in cleaned]\n",
    "    cleaned = [mystr.replace('_', ' ') for mystr in cleaned]\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_cols(data):\n",
    "    data = lower_cols(data)\n",
    "    data = remove_chars(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(pandas_dataset, df_target):\n",
    "    if (not pandas_dataset.empty):\n",
    "        organization = 'HDX'   #Replace if datasets contains organization\n",
    "        pandas_dataset.dropna(how = 'all', inplace = True)\n",
    "        pandas_dataset.dropna(axis=1, how = 'all', subset=range(1,len(pandas_dataset)), inplace = True)\n",
    "        headers = list(pandas_dataset.columns.values)        \n",
    "        headers = clean_cols(headers)\n",
    "    for i in range(len(headers)):\n",
    "        try:\n",
    "            dic = {'Header': headers[i], \n",
    "                   'Data': list(pandas_dataset.iloc[1:, i]), \n",
    "                   'Relative Column Position': (i+1) / len(pandas_dataset.columns), \n",
    "                   'Organization': organization,\n",
    "                   'Index': i}\n",
    "            df_target.loc[len(df_target)] = dic\n",
    "        except:\n",
    "            raise Exception(\"Error: arguments not matched\")\n",
    "    df_result = transform_vectorizers(df_target)\n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_vectorizers(df_target):\n",
    "    number_of_data_point_to_vectorize = 7\n",
    "    cols = ['Header_embedding', 'Organization_embedded', 'features_combined']\n",
    "    df = pd.DataFrame(columns = cols)\n",
    "    print(df_target.head())\n",
    "    df_target, number_of_data_point_to_vectorize = embedded_datapoints(df_target, 7)\n",
    "    df['data_combined'] = df_target.loc[:, 'embedded_datapoint0': 'embedded_datapoint' \n",
    "                                                           + str(number_of_data_point_to_vectorize-1)].values.tolist()\n",
    "    df['data_combined'] = df['data_combined'].apply(lambda x: [val for item in x for val in item])\n",
    "    df['Header_embedding'] = df_target['Header'].astype(str).apply(fmodel.get_sentence_vector)\n",
    "    df['Organization_embedded'] = df_target['Organization'].astype(str).apply(fmodel.get_sentence_vector)\n",
    "    cols = ['Header_embedding', 'Organization_embedded', 'data_combined']\n",
    "    df['features_combined'] = df[cols].values.tolist()\n",
    "    df['features_combined'] = df['features_combined'].apply(lambda x: [val for item in x for val in item])\n",
    "    diff = 2700 - len(df['features_combined'][0])\n",
    "    for i in range(len(df)):\n",
    "        for j in range(diff):\n",
    "            df['features_combined'][i].append(0)\n",
    "    df = df.dropna()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_words(series): \n",
    "    #each series is a long string that contains all the data\n",
    "    lst = []\n",
    "    cleanlist = [str(x) for x in series if str(x) != 'nan']\n",
    "    for i in cleanlist:\n",
    "        lst = re.split(r\"\\W+\", i)\n",
    "        lst.extend(list(filter(None, lst)))\n",
    "    return lst\n",
    "    \n",
    "def vectorize_n_datapoints(df, number_of_datapoints_to_vectorize = 7):\n",
    "#     print(df['Data'].head())\n",
    "#     print(df['Data'].iloc[0])\n",
    "#     for i in range(len(df['Data'])):\n",
    "#         df['Data_separated'].iloc[0] = separate_words(df['Data'].iloc[0])\n",
    "    df['Data_separated'] = df['Data'].apply(separate_words)\n",
    "    if (number_of_datapoints_to_vectorize > len(df['Data_separated'][0])):\n",
    "        number_of_datapoints_to_vectorize = len(df['Data_separated'][0])\n",
    "    for i in range(number_of_datapoints_to_vectorize):\n",
    "        df['datapoint' + str(i)] = df['Data_separated'].str[i]\n",
    "    return df, number_of_datapoints_to_vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedded_datapoints(df, number_of_data_point_to_vectorize=7):\n",
    "    df, number_of_data_point_to_vectorize = vectorize_n_datapoints(df)\n",
    "    for i in range(number_of_data_point_to_vectorize):\n",
    "        \n",
    "        df['embedded_datapoint' + str(i)] = df['datapoint' + str(i)].map(lambda x: fmodel.get_sentence_vector(str(x)))\n",
    "    return df, number_of_data_point_to_vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(data_lst):\n",
    "    #remove stopwords from the data including 'the', 'and' etc.\n",
    "    wordsFiltered = []\n",
    "    for w in data_lst:\n",
    "        if w not in stopWords:\n",
    "            wordsFiltered.append(w)\n",
    "    return wordsFiltered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_extract(row):\n",
    "    ignore = ['nan']\n",
    "    no_white = [i.lstrip() for i in row if i not in ignore and not (isinstance(i, float) or isinstance(i,int))]\n",
    "    cleaned_text = [w.lower() for w in no_white if w not in ignore]\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def allowed_file_csv(filename):\n",
    "    return '.' in filename and \\\n",
    "           filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS_CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def allowed_file_json(filename):\n",
    "    return '.' in filename and \\\n",
    "            filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS_JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_n_grams(data_lst, n):\n",
    "    # cleaned = remove_chars(list(data_lst))\n",
    "    # cleaned = clean_cols(cleaned)\n",
    "    cleaned = remove_stop_words(data_lst)\n",
    "    #make sure that n_grams 'refresh' when a new dataset is encountered!!!!   \n",
    "    return list(ngrams(cleaned, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dataset = pd.read_csv('Cluster 3ws - Protection.csv', na_values=['nan',' nan'])\n",
    "# input_dataset = input_dataset.rename(columns=input_dataset.iloc[0]).drop(input_dataset.index[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          Header  \\\n",
      "0              lead organization   \n",
      "1    organization type lead org.   \n",
      "2           implementing partner   \n",
      "3  organization type imp.partner   \n",
      "4                 sector/cluster   \n",
      "\n",
      "                                                Data  \\\n",
      "0  [UNHCR, WFP, UN Women, UN Women, IOM, PNDH, PN...   \n",
      "1  [Agências das Nações Unidas, Agências das Naçõ...   \n",
      "2  [nan, nan, Gender Links, Gender Links, PGR, na...   \n",
      "3  [nan, nan, ONGs internacionais, ONGs internaci...   \n",
      "4  [Proteção, Proteção, Proteção, Proteção, Prote...   \n",
      "\n",
      "   Relative Column Position Organization Index  \n",
      "0                  0.047619          HDX     0  \n",
      "1                  0.095238          HDX     1  \n",
      "2                  0.142857          HDX     2  \n",
      "3                  0.190476          HDX     3  \n",
      "4                  0.238095          HDX     4  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vanis\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:251: UserWarning: Trying to unpickle estimator LabelBinarizer from version 0.19.0 when using version 0.20.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "C:\\Users\\vanis\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:251: UserWarning: Trying to unpickle estimator MLPClassifier from version 0.19.0 when using version 0.20.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>affected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>affected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>affected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>affected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sector</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>affected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>meta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>affected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>affected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>adm2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>adm2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>adm2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>adm2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>affected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>adm2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>country</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>affected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>affected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>date</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>date</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>status</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0\n",
       "0   affected\n",
       "1   affected\n",
       "2   affected\n",
       "3   affected\n",
       "4     sector\n",
       "5   affected\n",
       "6       meta\n",
       "7   affected\n",
       "8   affected\n",
       "9       adm2\n",
       "10      adm2\n",
       "11      adm2\n",
       "12      adm2\n",
       "13  affected\n",
       "14      adm2\n",
       "15   country\n",
       "16  affected\n",
       "17  affected\n",
       "18      date\n",
       "19      date\n",
       "20    status"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#                 # process the untagged dataset\n",
    "processed_dataset = preprocess(input_dataset, \n",
    "                               pd.DataFrame(columns=['Header','Data','Relative Column Position','Organization','Index']))\n",
    "model = pickle.load(open(\"model.pkl\", \"rb\")) #Model needs be named model.pkl, preferably using version 0.20.3\n",
    "output_dataset = pd.DataFrame(data = model.predict(list(processed_dataset['features_combined'])))\n",
    "output_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @app.route('/', methods=['GET','POST'])\n",
    "# def upload_file():\n",
    "#     if request.method == 'POST':\n",
    "#         # check if the post request has the file part\n",
    "#         if 'file' not in request.files:\n",
    "#             flash('No file part')\n",
    "#             return redirect(request.url)\n",
    "#         file = request.files['file']\n",
    "        \n",
    "#         if file.filename == '':\n",
    "#             # flash('No selected file')\n",
    "#             return redirect(request.url)\n",
    "#         # file.save(os.getcwd())\n",
    "#         if file and allowed_file_csv(file.filename):\n",
    "#             filename = secure_filename(file.filename)\n",
    "#             input_dataset = pd.read_csv(file)\n",
    "                \n",
    "#         if file and allowed_file_json(file.filename):\n",
    "#             # filename = secure_filename(file.filename)\n",
    "#             input_dataset = pd.read_json(file)\n",
    "#             input_dataset = input_dataset.rename(columns=input_dataset.iloc[0]).drop(input_dataset.index[0])\n",
    "#                 # process the untagged dataset\n",
    "#         processed_dataset = preprocess(input_dataset, \n",
    "#             pd.DataFrame(columns=['Header','Data','Relative Column Position','Organization','Index']))\n",
    "#         model = pickle.load(open(\"model.pkl\", \"rb\")) #Model needs be named model.pkl, preferably using version 0.20.3\n",
    "#         output_dataset = pd.DataFrame(data = model.predict(list(processed_dataset['features_combined'])))\n",
    "#         resp = make_response(output_dataset.to_csv())\n",
    "#         resp.headers[\"Content-Disposition\"] = \"attachment; filename=export.csv\"\n",
    "#         resp.headers[\"Content-Type\"] = \"text/csv\"\n",
    "#         return resp\n",
    "        \n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    return \n",
    "<br>\n",
    "    <!doctype html><br>\n",
    "    <title>Upload new File</title><br>\n",
    "    <h1>Upload new File (only CSV and JSON files accepted)</h1><br>\n",
    "    <form method=post enctype=multipart/form-data><br>\n",
    "      <input type=file name=file><br>\n",
    "      <input type=submit value=Upload><br>\n",
    "    # <form method=post><br>\n",
    "    #   <input name=text><br>\n",
    "    #   <input type=submit><br>\n",
    "    </form><br>\n",
    "  \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "     app.run(debug=True)\n",
    "     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
